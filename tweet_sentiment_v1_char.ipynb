{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T21:26:52.585281Z",
     "start_time": "2020-06-15T21:26:52.253867Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T21:26:53.600025Z",
     "start_time": "2020-06-15T21:26:52.631328Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "### Torch Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T21:27:01.342699Z",
     "start_time": "2020-06-15T21:27:01.257456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641555ac70</td>\n",
       "      <td>Hmmm, maybe that`s what they meant. They elud...</td>\n",
       "      <td>Hmmm, maybe that`s what they meant. They elude...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d618862cbd</td>\n",
       "      <td>i want magic mountain tix but i dont get ur s...</td>\n",
       "      <td>o sux</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8b73cf5d5</td>\n",
       "      <td>Screw you guys and your prom pictures</td>\n",
       "      <td>Screw you guys</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ff62cd38d</td>\n",
       "      <td>Coffee in hand and sun shining in my window.  ...</td>\n",
       "      <td>Hope everyone`s having a great Monday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4381295572</td>\n",
       "      <td>Off to dinner with  &amp; his fam.</td>\n",
       "      <td>is fam.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  641555ac70   Hmmm, maybe that`s what they meant. They elud...   \n",
       "1  d618862cbd   i want magic mountain tix but i dont get ur s...   \n",
       "2  a8b73cf5d5              Screw you guys and your prom pictures   \n",
       "3  2ff62cd38d  Coffee in hand and sun shining in my window.  ...   \n",
       "4  4381295572                     Off to dinner with  & his fam.   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0  Hmmm, maybe that`s what they meant. They elude...   neutral  \n",
       "1                                              o sux  negative  \n",
       "2                                     Screw you guys  negative  \n",
       "3              Hope everyone`s having a great Monday  positive  \n",
       "4                                            is fam.  positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_train.dropna(inplace=True)\n",
    "df_train, df_val = train_test_split(df_train, test_size=3000)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda x: x.lower())\n",
    "df_train['selected_text'] = df_train['selected_text'].apply(lambda x: x.lower())\n",
    "\n",
    "df_val['text'] = df_val['text'].apply(lambda x: x.lower())\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All unique characters (Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = {'<PAD>':0, 'UNK':1, '<sos>':2, '<eos>':3}\n",
    "chars = ['<PAD>', 'UNK', '<sos>', '<eos>']\n",
    "for c in set(' '.join(df_train.text)):\n",
    "    vocab2idx[c] = len(chars)\n",
    "    chars.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['encoded_text'] = df_train.text.apply(lambda x: [vocab2idx['<sos>']]+[vocab2idx.get(c, vocab2idx['UNK']) for c in list(x)]+[vocab2idx['<eos>']])\n",
    "df_val['encoded_text'] = df_val.text.apply(lambda x: [vocab2idx['<sos>']]+[vocab2idx.get(c, vocab2idx['UNK']) for c in list(x)]+[vocab2idx['<eos>']])\n",
    "df_test['encoded_text'] = df_test.text.apply(lambda x: [vocab2idx['<sos>']]+[vocab2idx.get(c, vocab2idx['UNK']) for c in list(x)]+[vocab2idx['<eos>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['encoded_selected'] = df_train.selected_text.apply(lambda x: [vocab2idx['<sos>']]+[vocab2idx.get(c, vocab2idx['UNK']) for c in list(x)]+[vocab2idx['<eos>']])\n",
    "df_val['encoded_selected'] = df_val.selected_text.apply(lambda x: [vocab2idx['<sos>']]+[vocab2idx.get(c, vocab2idx['UNK']) for c in list(x)]+[vocab2idx['<eos>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_le = LabelEncoder().fit(df_train.sentiment)\n",
    "df_train['encode_sent'] = sent_le.transform(df_train.sentiment)\n",
    "df_val['encode_sent'] = sent_le.transform(df_val.sentiment)\n",
    "df_test['encode_sent'] = sent_le.transform(df_test.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>encoded_selected</th>\n",
       "      <th>encode_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641555ac70</td>\n",
       "      <td>hmmm, maybe that`s what they meant. they elud...</td>\n",
       "      <td>hmmm, maybe that`s what they meant. they elude...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[2, 74, 48, 12, 12, 12, 42, 74, 12, 54, 26, 68...</td>\n",
       "      <td>[2, 48, 12, 12, 12, 42, 74, 12, 54, 26, 68, 60...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d618862cbd</td>\n",
       "      <td>i want magic mountain tix but i dont get ur s...</td>\n",
       "      <td>o sux</td>\n",
       "      <td>negative</td>\n",
       "      <td>[2, 74, 37, 74, 61, 54, 10, 75, 74, 12, 54, 19...</td>\n",
       "      <td>[2, 78, 74, 56, 38, 62, 3]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8b73cf5d5</td>\n",
       "      <td>screw you guys and your prom pictures</td>\n",
       "      <td>screw you guys</td>\n",
       "      <td>negative</td>\n",
       "      <td>[2, 56, 20, 13, 60, 61, 74, 26, 78, 38, 74, 19...</td>\n",
       "      <td>[2, 56, 20, 13, 60, 61, 74, 26, 78, 38, 74, 19...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2ff62cd38d</td>\n",
       "      <td>coffee in hand and sun shining in my window.  ...</td>\n",
       "      <td>hope everyone`s having a great monday</td>\n",
       "      <td>positive</td>\n",
       "      <td>[2, 20, 78, 63, 63, 60, 60, 74, 37, 10, 74, 48...</td>\n",
       "      <td>[2, 48, 78, 67, 60, 74, 60, 49, 60, 13, 26, 78...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4381295572</td>\n",
       "      <td>off to dinner with  &amp; his fam.</td>\n",
       "      <td>is fam.</td>\n",
       "      <td>positive</td>\n",
       "      <td>[2, 78, 63, 63, 74, 75, 78, 74, 8, 37, 10, 10,...</td>\n",
       "      <td>[2, 37, 56, 74, 63, 54, 12, 15, 3]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  641555ac70   hmmm, maybe that`s what they meant. they elud...   \n",
       "1  d618862cbd   i want magic mountain tix but i dont get ur s...   \n",
       "2  a8b73cf5d5              screw you guys and your prom pictures   \n",
       "3  2ff62cd38d  coffee in hand and sun shining in my window.  ...   \n",
       "4  4381295572                     off to dinner with  & his fam.   \n",
       "\n",
       "                                       selected_text sentiment  \\\n",
       "0  hmmm, maybe that`s what they meant. they elude...   neutral   \n",
       "1                                              o sux  negative   \n",
       "2                                     screw you guys  negative   \n",
       "3              hope everyone`s having a great monday  positive   \n",
       "4                                            is fam.  positive   \n",
       "\n",
       "                                        encoded_text  \\\n",
       "0  [2, 74, 48, 12, 12, 12, 42, 74, 12, 54, 26, 68...   \n",
       "1  [2, 74, 37, 74, 61, 54, 10, 75, 74, 12, 54, 19...   \n",
       "2  [2, 56, 20, 13, 60, 61, 74, 26, 78, 38, 74, 19...   \n",
       "3  [2, 20, 78, 63, 63, 60, 60, 74, 37, 10, 74, 48...   \n",
       "4  [2, 78, 63, 63, 74, 75, 78, 74, 8, 37, 10, 10,...   \n",
       "\n",
       "                                    encoded_selected  encode_sent  \n",
       "0  [2, 48, 12, 12, 12, 42, 74, 12, 54, 26, 68, 60...            1  \n",
       "1                         [2, 78, 74, 56, 38, 62, 3]            0  \n",
       "2  [2, 56, 20, 13, 60, 61, 74, 26, 78, 38, 74, 19...            0  \n",
       "3  [2, 48, 78, 67, 60, 74, 60, 49, 60, 13, 26, 78...            2  \n",
       "4                 [2, 37, 56, 74, 63, 54, 12, 15, 3]            2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets(Dataset):\n",
    "    def __init__(self, df, tv=True):\n",
    "        self.X = df.encoded_text.values\n",
    "        self.sent = df.encode_sent.values\n",
    "        self.y = df.encoded_selected.values if tv else np.zeros((len(self.X), 1))\n",
    "    \n",
    "    def __len__(self): return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.X[idx]), torch.Tensor(self.y[idx]), torch.Tensor([self.sent[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Tweets(df_train)\n",
    "val_ds = Tweets(df_val)\n",
    "test_ds = Tweets(df_test, tv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2idx[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2., 74., 48., 12., 12., 12., 42., 74., 12., 54., 26., 68., 60., 74.,\n",
       "         75., 48., 54., 75., 69., 56., 74., 61., 48., 54., 75., 74., 75., 48.,\n",
       "         60., 26., 74., 12., 60., 54., 10., 75., 15., 74., 75., 48., 60., 26.,\n",
       "         74., 60., 45., 38.,  8., 60.,  8., 74., 75., 78., 74., 56., 78., 12.,\n",
       "         60., 75., 48., 37., 10., 19., 74., 68., 13., 54., 10.,  8., 74., 10.,\n",
       "         60., 61., 74., 68., 38., 75., 74., 26., 78., 38., 74., 18., 10., 78.,\n",
       "         61., 74., 48., 78., 61., 74., 75., 48., 60., 74., 12., 60.,  8., 37.,\n",
       "         54., 74., 37., 56.,  3.]),\n",
       " tensor([ 2., 48., 12., 12., 12., 42., 74., 12., 54., 26., 68., 60., 74., 75.,\n",
       "         48., 54., 75., 69., 56., 74., 61., 48., 54., 75., 74., 75., 48., 60.,\n",
       "         26., 74., 12., 60., 54., 10., 75., 15., 74., 75., 48., 60., 26., 74.,\n",
       "         60., 45., 38.,  8., 60.,  8., 74., 75., 78., 74., 56., 78., 12., 60.,\n",
       "         75., 48., 37., 10., 19., 74., 68., 13., 54., 10.,  8., 74., 10., 60.,\n",
       "         61., 74., 68., 38., 75., 74., 26., 78., 38., 74., 18., 10., 78., 61.,\n",
       "         74., 48., 78., 61., 74., 75., 48., 60., 74., 12., 60.,  8., 37., 54.,\n",
       "         74., 37., 56.,  3.]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    (X, y, s) = zip(*batch)\n",
    "    x_len = [len(x) for x in X]\n",
    "    x_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_pad = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    return x_pad, x_len, y_pad, torch.Tensor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, sentiment):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.sentiment = nn.Embedding(sentiment, hidden_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        \n",
    "    def forward(self, x, lengths, sentiment):\n",
    "        x = self.dropout(self.vocabs(x.long()))\n",
    "        x_pack = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "        return hidden[-1] + self.sentiment(sentiment.long()), cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.vocabs(x.long())\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        return self.linear(hidden[-1]), hidden, cell  #NOTE: hidden[-1] returns everything within that batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(encoder, decoder, enc_optimizer, dec_optimizer, x, y, s, lengths,\n",
    "                teacher_forcing_ratio=0.5):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # **ENCODER**\n",
    "    hidden, cell = encoder(x, lengths, s)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        y_idx = y[:, idx]\n",
    "        loss += F.cross_entropy(output, y_idx.long(), ignore_index=0)\n",
    "        teacher_force = True if np.random.uniform() > teacher_forcing_ratio else False\n",
    "        if teacher_force:\n",
    "            decoder_input = y_idx.unsqueeze(1)\n",
    "        else:\n",
    "            decoder_input = output.argmax(dim=1).unsqueeze(1)\n",
    "    \n",
    "    # updating the gradient\n",
    "    loss.backward()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_batch(encoder, decoder, x, y, s, lengths,\n",
    "                teacher_forcing_ratio=0.5):\n",
    "    loss = 0\n",
    "    # **ENCODER**\n",
    "    hidden, cell = encoder(x, lengths, s)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        y_idx = y[:, idx]\n",
    "        loss += F.cross_entropy(output, y_idx.long(), ignore_index=0)\n",
    "        teacher_force = True if np.random.uniform() > teacher_forcing_ratio else False\n",
    "        if teacher_force:\n",
    "            decoder_input = y_idx.unsqueeze(1)\n",
    "        else:\n",
    "            decoder_input = output.argmax(dim=1).unsqueeze(1)\n",
    "    \n",
    "    # updating the gradient\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, val_dl, epochs=10):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = time.time()\n",
    "        total_loss, total = 0, 0\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for x, lengths, y, s in train_dl:\n",
    "            loss = train_batch(encoder, decoder, enc_optimizer, dec_optimizer, x.to(device), y.to(device), s.to(device), lengths)\n",
    "            total_loss += loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        total_v, total_loss_v = 0, 0\n",
    "        for x, lengths, y, s in val_dl:\n",
    "            loss = val_batch(encoder, decoder, x.to(device), y.to(device), s.to(device), lengths)\n",
    "            total_loss_v += loss*x.size(0)\n",
    "            total_v += x.size(0)\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} Training Loss: {total_loss/total} Val Loss: {total_loss_v/total_v} Time: {time.time()-start:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (vocabs): Embedding(79, 100)\n",
       "  (lstm): LSTM(100, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=79, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(len(vocab2idx), 100, 50, 3)\n",
    "dec = Decoder(len(vocab2idx), 100, 50)\n",
    "enc_optimizer = optim.Adam(enc.parameters(), lr=3e-4)\n",
    "dec_optimizer = optim.Adam(dec.parameters(), lr=3e-4)\n",
    "enc.to(device)\n",
    "dec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sthodla/anaconda3/envs/panda/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2f6762bb4549b1bc43e74722267ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 523.6732610166463 Val Loss: 459.45677604166667 Time: 23.829\n",
      "Epoch: 2 Training Loss: 436.15857591816024 Val Loss: 451.70537076822916 Time: 23.572\n",
      "Epoch: 3 Training Loss: 430.2331030353222 Val Loss: 449.9190830891927 Time: 23.544\n",
      "Epoch: 4 Training Loss: 425.4427426805683 Val Loss: 447.07378686523435 Time: 24.632\n",
      "Epoch: 5 Training Loss: 422.061186268127 Val Loss: 444.1272044270833 Time: 25.432\n",
      "Epoch: 6 Training Loss: 418.4986506044475 Val Loss: 441.6062579752604 Time: 23.183\n",
      "Epoch: 7 Training Loss: 414.835333850337 Val Loss: 438.0980519205729 Time: 23.175\n",
      "Epoch: 8 Training Loss: 412.45414177389705 Val Loss: 435.61573706054685 Time: 23.314\n",
      "Epoch: 9 Training Loss: 409.17613553315203 Val Loss: 435.634916015625 Time: 23.093\n",
      "Epoch: 10 Training Loss: 406.6850461473652 Val Loss: 432.9018920898437 Time: 23.097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(enc, dec, enc_optimizer, dec_optimizer, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
