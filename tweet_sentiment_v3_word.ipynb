{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:10.809865Z",
     "start_time": "2020-06-19T04:40:10.804834Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:12.746236Z",
     "start_time": "2020-06-19T04:40:11.477826Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Torch Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:12.784936Z",
     "start_time": "2020-06-19T04:40:12.747991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:12.967481Z",
     "start_time": "2020-06-19T04:40:12.786391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:12.977769Z",
     "start_time": "2020-06-19T04:40:12.968897Z"
    }
   },
   "outputs": [],
   "source": [
    "# index 314 has no text\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:13.737492Z",
     "start_time": "2020-06-19T04:40:13.721732Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val, _, _ = train_test_split(df, \n",
    "                                          df['selected_text'], \n",
    "                                          test_size=.05, \n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:13.822328Z",
     "start_time": "2020-06-19T04:40:13.771001Z"
    }
   },
   "outputs": [],
   "source": [
    "# 26k samples in training to 1k samples in validation\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "* Lowercase - possible since the predicted sentiment text and the selected_text will be lowercase when computing metric.\n",
    "* punctuation - keep the punctuation given that the submission file states that need to be quoted and complete. \n",
    "* Numericalize - Turn each token into its corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:14.783496Z",
     "start_time": "2020-06-19T04:40:14.777865Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    This function will preprocess the input sentence sequence to avoid any further preprocessing\n",
    "    downstream.\n",
    "    \"\"\"\n",
    "    return sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:15.393418Z",
     "start_time": "2020-06-19T04:40:15.081855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bebc86b81f4b8fa0549da43330d268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26106.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689e404b31544852a18bc738acc48c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603b65d4ce2b4b1da62a711e099f0f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26106.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7853d8ddd3b7456ebb5f7e538247fad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ed134567c403b920b2a223609ade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3534.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lowercasing all the text and turning them into a list of tokens for text and selected text in the training set\n",
    "df_train['text'] = df_train['text'].progress_apply(preprocessing)\n",
    "df_val['text'] = df_val['text'].progress_apply(preprocessing)\n",
    "\n",
    "df_train['selected_text'] = df_train['selected_text'].progress_apply(preprocessing)\n",
    "df_val['selected_text'] = df_val['selected_text'].progress_apply(preprocessing)\n",
    "\n",
    "df_test['text'] = df_test['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:15.565646Z",
     "start_time": "2020-06-19T04:40:15.546958Z"
    }
   },
   "outputs": [],
   "source": [
    "# making the sentiment to a variable\n",
    "df_train['sentiment'] = df_train['sentiment'].astype('category')\n",
    "df_train['code'] = df_train['sentiment'].cat.codes\n",
    "\n",
    "df_val['sentiment'] = df_val['sentiment'].astype('category')\n",
    "df_val['code'] = df_val['sentiment'].cat.codes\n",
    "\n",
    "df_test['sentiment'] = df_test['sentiment'].astype('category')\n",
    "df_test['code'] = df_test['sentiment'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:16.404179Z",
     "start_time": "2020-06-19T04:40:16.398566Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_words(data):\n",
    "    \"\"\"\n",
    "    Find the number of unique words in the training set.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for text in data:\n",
    "        for word in text:\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:16.915024Z",
     "start_time": "2020-06-19T04:40:16.855424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43859"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_words = unique_words(df_train.text)\n",
    "len(unq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:17.443528Z",
     "start_time": "2020-06-19T04:40:17.438832Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(words):\n",
    "    vocab2idx, idx = {}, 4\n",
    "    vocab2idx['<pad>'] = 0\n",
    "    vocab2idx['<unk>'] = 1\n",
    "    vocab2idx['<sos>'] = 2\n",
    "    vocab2idx['<eos>'] = 3\n",
    "    \n",
    "    for word in words:\n",
    "        vocab2idx[word] = idx\n",
    "        idx += 1\n",
    "    return vocab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:40:18.443488Z",
     "start_time": "2020-06-19T04:40:18.423843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43863"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2idx = create_vocabulary(unq_words)\n",
    "len(vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:45:22.706019Z",
     "start_time": "2020-06-19T04:45:22.699290Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoding_with_no_padding(sentence, vocab2idx):\n",
    "    numericalize = [vocab2idx['<sos>']]\n",
    "    for token in sentence:\n",
    "        numericalize.append(vocab2idx.get(token, vocab2idx['<unk>']))\n",
    "    numericalize.append(vocab2idx['<eos>'])\n",
    "    return numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:50:15.950567Z",
     "start_time": "2020-06-19T04:50:15.940835Z"
    }
   },
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self, data, vocab2idx):\n",
    "        self.X = [encoding_with_no_padding(x, vocab2idx) for x in data['text']]\n",
    "        self.y = [encoding_with_no_padding(y, vocab2idx) for y in data['selected_text']]\n",
    "        #self.sentiment = data.code.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.X[idx]), torch.LongTensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:50:19.411034Z",
     "start_time": "2020-06-19T04:50:19.111693Z"
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "train_ds = tweetDataset(df_train, vocab2idx)\n",
    "valid_ds = tweetDataset(df_val, vocab2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:50:25.600429Z",
     "start_time": "2020-06-19T04:50:25.592937Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    (X, y, s) = zip(*batch)\n",
    "    x_len = [len(x) for x in X]\n",
    "    x_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_pad = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    return x_pad, x_len, y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T04:45:27.740092Z",
     "start_time": "2020-06-19T04:45:27.722410Z"
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "batch_size = 3\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "x, lengths, y, s = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:02:07.954430Z",
     "start_time": "2020-06-19T07:02:07.944609Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, sentiment):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.sentiment = nn.Embedding(sentiment, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.dropout(self.vocabs(x))\n",
    "        x_pack = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "        return out, torch.flatten(hidden.transpose(1,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:02:08.409931Z",
     "start_time": "2020-06-19T07:02:08.386408Z"
    }
   },
   "outputs": [],
   "source": [
    "#got rid of multi layer\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 1, df_train.code.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:02:08.998608Z",
     "start_time": "2020-06-19T07:02:08.985395Z"
    }
   },
   "outputs": [],
   "source": [
    "out, h = encoder(x, lengths) # h has dimensions [batch_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:02:09.410306Z",
     "start_time": "2020-06-19T07:02:09.403941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:44:55.629447Z",
     "start_time": "2020-06-19T06:44:55.626074Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.randn(3, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:45:11.165994Z",
     "start_time": "2020-06-19T06:45:11.160396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t, h), dim=1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:51:14.473460Z",
     "start_time": "2020-06-19T06:51:14.469329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64]), torch.Size([3, 64]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:22:52.364033Z",
     "start_time": "2020-06-19T06:22:52.359402Z"
    }
   },
   "outputs": [],
   "source": [
    "# unsqueeze and permute so matrix multiplication can be done\n",
    "h = h.unsqueeze(0)\n",
    "h = h.permute((1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:22:53.401527Z",
     "start_time": "2020-06-19T06:22:53.398568Z"
    }
   },
   "outputs": [],
   "source": [
    "res = torch.bmm(out, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:22:58.976899Z",
     "start_time": "2020-06-19T06:22:58.973124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 17, 1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:25:27.203452Z",
     "start_time": "2020-06-19T06:25:27.198793Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = F.softmax(res.squeeze(), dim=1)  ## dim of (3, 17) [batch, seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:25:32.239619Z",
     "start_time": "2020-06-19T06:25:32.234010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 17])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:25:34.710134Z",
     "start_time": "2020-06-19T06:25:34.705944Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attention = torch.bmm(alpha.unsqueeze(1),out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:54:26.401400Z",
     "start_time": "2020-06-19T06:54:26.397385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:09:59.801834Z",
     "start_time": "2020-06-19T06:09:59.795083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 17, 128]), torch.Size([3, 17]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size(), alpha.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T05:41:09.218609Z",
     "start_time": "2020-06-19T05:41:09.101718Z"
    }
   },
   "outputs": [],
   "source": [
    "dec = nn.LSTM(len(vocab2idx), 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T06:55:44.643897Z",
     "start_time": "2020-06-19T06:55:44.631812Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell, enc_hidden):\n",
    "        x = self.vocabs(x)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        hidden = hidden.unsqueeze(0).permute((1,2,0))\n",
    "        scores = torch.bmm(enc_hidden, hidden)\n",
    "        alpha = F.softmax(scores.squeeze(), dim=1)\n",
    "        attention = torch.bmm(alpha.unsqueeze(1), enc_hidden).squeeze(1)\n",
    "        final = torch.cat((attention, hidden), dim=1)\n",
    "        return self.linear(final[-1]), hidden, cell  #NOTE: hidden[-1] returns everything within that batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:02:45.272590Z",
     "start_time": "2020-06-19T07:02:45.144444Z"
    }
   },
   "outputs": [],
   "source": [
    "#got rid of multi layer\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 1, df_train.code.nunique())\n",
    "decoder = AttentionDecoder(len(vocab2idx), 64, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:03:02.280069Z",
     "start_time": "2020-06-19T07:03:02.267936Z"
    }
   },
   "outputs": [],
   "source": [
    "out, h = encoder(x, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:02:18.881585Z",
     "start_time": "2020-06-19T03:02:18.866094Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "                e_path, d_path, tf_ratio, epochs=10, save_val=False):\n",
    "    best_val = float('inf')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = time.time()\n",
    "        total_loss, total = 0, 0\n",
    "        val_loss, total_v = 0, 0\n",
    "        ### Training\n",
    "        for x, lengths, y, s in train_dl:\n",
    "            loss = train_batch(encoder, \n",
    "                               decoder, \n",
    "                               enc_optimizer, \n",
    "                               dec_optimizer, \n",
    "                               x.to(device), \n",
    "                               y.to(device), \n",
    "                               s.to(device), \n",
    "                               lengths, \n",
    "                               tf=True,\n",
    "                               tf_ratio=tf_ratio)\n",
    "            total_loss += loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        ### Validation\n",
    "        for x, lengths, y, s in valid_dl:\n",
    "            v_loss = train_batch(encoder, \n",
    "                                 decoder, \n",
    "                                 enc_optimizer, \n",
    "                                 dec_optimizer, \n",
    "                                 x.to(device), \n",
    "                                 y.to(device), \n",
    "                                 s.to(device), \n",
    "                                 lengths, \n",
    "                                 tf=False, \n",
    "                                 train=False)\n",
    "            val_loss += v_loss*x.size(0)\n",
    "            total_v += x.size(0)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1} Train Loss: {total_loss/total:.3f} Val Loss: {val_loss/total_v:.3f} Time: {time.time()-start:.3f}\")\n",
    "        \n",
    "        if save_val:\n",
    "            if best_val > (val_loss/total_v):\n",
    "                save_model(encoder, decoder, e_path, d_path)\n",
    "                best_val = val_loss/total_v\n",
    "        else:\n",
    "            if best_val > (total_loss/total):\n",
    "                save_model(encoder, decoder, e_path, d_path)\n",
    "                best_val = total_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:02:20.786491Z",
     "start_time": "2020-06-19T03:02:20.772700Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(encoder, decoder, enc_optimizer, dec_optimizer, x, y, s, lengths, tf,\n",
    "                train=True, tf_ratio=0.5):\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "    \n",
    "    # zero grad for both optimizers\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # **ENCODER**\n",
    "    hidden = encoder(x, lengths, s)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        y_idx = y[:, idx]\n",
    "        loss += F.cross_entropy(output, y_idx, ignore_index=0)\n",
    "        # if teacher forcing\n",
    "        if tf:\n",
    "            teacher_force = True if np.random.uniform() > tf_ratio else False\n",
    "            if teacher_force:\n",
    "                decoder_input = y_idx.unsqueeze(1)\n",
    "        else:\n",
    "            decoder_input = output.argmax(dim=1).unsqueeze(1)\n",
    "            \n",
    "    # updating the gradient\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:02:22.449426Z",
     "start_time": "2020-06-19T03:02:22.443688Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, e_path, d_path):\n",
    "    torch.save(encoder.state_dict(), e_path)\n",
    "    torch.save(decoder.state_dict(), d_path)\n",
    "    \n",
    "def load_model(encoder, decoder, e_path, d_path):\n",
    "    encoder.load_state_dict(torch.load(e_path))\n",
    "    decoder.load_state_dict(torch.load(d_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T00:42:58.514494Z",
     "start_time": "2020-06-19T00:42:58.360305Z"
    }
   },
   "outputs": [],
   "source": [
    "# purposedly saving overfit model to test for sentence quality\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 2, len(df_train.code.unique())).to(device)\n",
    "decoder = Decoder(len(vocab2idx), 64, 288).to(device)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:09:27.517444Z",
     "start_time": "2020-06-19T03:09:27.218005Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_ds = tweetDataset(df_train, vocab2idx)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:34:14.297269Z",
     "start_time": "2020-06-19T00:49:31.725155Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da52f07ea6de46ebbbf1613b5f74a660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 147.639 Val Loss: 214.796 Time: 26.662\n",
      "Epoch: 20 Train Loss: 109.788 Val Loss: 240.359 Time: 26.797\n",
      "Epoch: 30 Train Loss: 86.114 Val Loss: 257.885 Time: 26.630\n",
      "Epoch: 40 Train Loss: 69.968 Val Loss: 281.450 Time: 26.796\n",
      "Epoch: 50 Train Loss: 58.125 Val Loss: 292.795 Time: 26.669\n",
      "Epoch: 60 Train Loss: 48.584 Val Loss: 312.429 Time: 26.633\n",
      "Epoch: 70 Train Loss: 40.244 Val Loss: 337.797 Time: 26.575\n",
      "Epoch: 80 Train Loss: 33.345 Val Loss: 361.842 Time: 26.618\n",
      "Epoch: 90 Train Loss: 27.753 Val Loss: 383.094 Time: 26.760\n",
      "Epoch: 100 Train Loss: 23.181 Val Loss: 405.456 Time: 26.628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models/multi_bidir/'\n",
    "train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "            model_path + 'bidir_multi_enc_lr_001_i32_h64.pth',\n",
    "            model_path + 'dec_lr_001_i64_h64.pth',\n",
    "            0,\n",
    "            100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:03:02.612837Z",
     "start_time": "2020-06-19T03:03:02.495145Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2vocab = {v:k for k, v in list(vocab2idx.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:03:02.973303Z",
     "start_time": "2020-06-19T03:03:02.966066Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(string1, string2):\n",
    "    a = set(string1.lower().split())\n",
    "    b = set(string2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:03:04.366746Z",
     "start_time": "2020-06-19T03:03:04.352175Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoding(encoder, decoder, x, y, s, lengths):\n",
    "    decoded_words = []\n",
    "    total_jaccard, total = 0, 0\n",
    "    \n",
    "    # **ENCODER**\n",
    "    hidden = encoder(x, lengths, s)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        pred = output.argmax(dim=1)\n",
    "        decoded_words.append(pred)\n",
    "        decoder_input = pred.unsqueeze(1)\n",
    "    \n",
    "    decoded_words = torch.cat([tensor.unsqueeze(0) for tensor in decoded_words]).transpose(1,0)\n",
    "    \n",
    "    for i in range(decoded_words.size(0)):\n",
    "        xi = decoded_words[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        x_sent = ' '.join([idx2vocab[idx] for idx in xi if idx > 3])\n",
    "        y_sent = ' '.join([idx2vocab[idx] for idx in yi if idx > 3])\n",
    "        total_jaccard += jaccard(x_sent, y_sent)\n",
    "        total += y.size(0)\n",
    "    return total_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:03:04.736068Z",
     "start_time": "2020-06-19T03:03:04.725675Z"
    }
   },
   "outputs": [],
   "source": [
    "def jac_scoring(encoder, decoder, data_loader):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    jac_score, total = 0, 0\n",
    "    \n",
    "    for x, lengths, y, s in tqdm(data_loader):\n",
    "        jac_score += decoding(encoder, decoder, x.to(device), y.to(device), s.to(device), lengths)\n",
    "        total += x.size(0)\n",
    "    print(f\"Jaccard Similarity: {jac_score/total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:41:31.888638Z",
     "start_time": "2020-06-19T01:41:22.884868Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8456cb55a17e42dfb7739fb7ad244967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard Similarity: 0.074\n"
     ]
    }
   ],
   "source": [
    "jac_scoring(encoder, decoder, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:45:12.559450Z",
     "start_time": "2020-06-19T01:45:12.554966Z"
    }
   },
   "outputs": [],
   "source": [
    "translation_train_dl = DataLoader(train_ds, batch_size=3, shuffle=True, collate_fn=collate)\n",
    "x, lengths, y, s = next(iter(translation_train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:03:09.861160Z",
     "start_time": "2020-06-19T03:03:09.849811Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, x, y, s, lengths):\n",
    "    decoded_words = []\n",
    "    total_jaccard, total = 0, 0\n",
    "    \n",
    "    # **ENCODER**\n",
    "    hidden = encoder(x, lengths, s)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell)\n",
    "        pred = output.argmax(dim=1)\n",
    "        decoded_words.append(pred)\n",
    "        decoder_input = pred.unsqueeze(1)\n",
    "    \n",
    "    decoded_words = torch.cat([tensor.unsqueeze(0) for tensor in decoded_words]).transpose(1,0)\n",
    "    \n",
    "    for i in range(decoded_words.size(0)):\n",
    "        xi = decoded_words[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        x_sent = ' '.join([idx2vocab[idx] for idx in xi if idx > 3])\n",
    "        y_sent = ' '.join([idx2vocab[idx] for idx in yi if idx > 3])\n",
    "        print(f\"Ground Truth\")\n",
    "        print()\n",
    "        print(y_sent)\n",
    "        print()\n",
    "        print(f\"Decoded Sentence\")\n",
    "        print(x_sent)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:51:06.935790Z",
     "start_time": "2020-06-19T01:51:06.910596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "concert next week cancelled\n",
      "\n",
      "Decoded Sentence\n",
      "\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "too many germï¿½n rodrï¿½guez`s results. which one are you? i mean, how do you look like in your profile picture?\n",
      "\n",
      "Decoded Sentence\n",
      "the fame ! . . . . it up and one off and some people will never be in that case\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "good\n",
      "\n",
      "Decoded Sentence\n",
      "good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this came from the bidirectional model at 100 epochs at 23.18 train loss\n",
    "translate(encoder, decoder, x.to(device), y.to(device), s.to(device), lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:51:21.661134Z",
     "start_time": "2020-06-19T01:51:21.454723Z"
    }
   },
   "outputs": [],
   "source": [
    "# purposedly saving overfit model to test for sentence quality\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 2, len(df_train.code.unique())).to(device)\n",
    "decoder = Decoder(len(vocab2idx), 64, 288).to(device)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=.001)\n",
    "load_model(encoder, decoder, \n",
    "           'models/multi_bidir/bidir_multi_enc_lr_001_i32_h64.pth', \n",
    "           'models/multi_bidir/dec_lr_001_i64_h64.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T01:51:29.924933Z",
     "start_time": "2020-06-19T01:51:29.901756Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "concert next week cancelled\n",
      "\n",
      "Decoded Sentence\n",
      "\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "too many germï¿½n rodrï¿½guez`s results. which one are you? i mean, how do you look like in your profile picture?\n",
      "\n",
      "Decoded Sentence\n",
      "the fame ! . . . . it up and one off and some people will never be in that case\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "good\n",
      "\n",
      "Decoded Sentence\n",
      "thanks for the day\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this came from the bidirectional model at 100 epochs at 23.18 train loss\n",
    "translate(encoder, decoder, x.to(device), y.to(device), s.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T02:36:30.750705Z",
     "start_time": "2020-06-19T01:51:51.682935Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119d83aed2ff4aa987324134afd8761b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 19.322 Val Loss: 429.665 Time: 26.717\n",
      "Epoch: 20 Train Loss: 17.307 Val Loss: 433.187 Time: 26.760\n",
      "Epoch: 30 Train Loss: 13.995 Val Loss: 474.280 Time: 26.693\n",
      "Epoch: 40 Train Loss: 13.573 Val Loss: 484.221 Time: 26.519\n",
      "Epoch: 50 Train Loss: 10.795 Val Loss: 507.179 Time: 26.672\n",
      "Epoch: 60 Train Loss: 10.727 Val Loss: 528.910 Time: 26.602\n",
      "Epoch: 70 Train Loss: 8.713 Val Loss: 541.374 Time: 26.617\n",
      "Epoch: 80 Train Loss: 7.929 Val Loss: 553.851 Time: 26.738\n",
      "Epoch: 90 Train Loss: 8.037 Val Loss: 563.733 Time: 26.657\n",
      "Epoch: 100 Train Loss: 6.791 Val Loss: 583.091 Time: 26.360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# continuous training\n",
    "train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "            model_path + 'bidir_multi_enc_lr_001_i32_h64_ct.pth',\n",
    "            model_path + 'dec_lr_001_i64_h64_ct.pth',\n",
    "            0,\n",
    "            100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:04:31.001800Z",
     "start_time": "2020-06-19T03:04:28.911620Z"
    }
   },
   "outputs": [],
   "source": [
    "# purposedly saving overfit model to test for sentence quality\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 2, len(df_train.code.unique())).to(device)\n",
    "decoder = Decoder(len(vocab2idx), 64, 288).to(device)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=.001)\n",
    "load_model(encoder, decoder, \n",
    "           'models/multi_bidir/bidir_multi_enc_lr_001_i32_h64_ct.pth',\n",
    "           'models/multi_bidir/dec_lr_001_i64_h64_ct.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:04:33.470144Z",
     "start_time": "2020-06-19T03:04:33.448145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "really wants a puppy...\n",
      "\n",
      "Decoded Sentence\n",
      "re-tweet. a.m! sofa ale\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "working 930a-730p today. 1.5 hour lunch..8.5 hours work. overtime = good. the fact that i`m already dead tired = very bad.\n",
      "\n",
      "Decoded Sentence\n",
      "big pos.. big planted). sj. amos 16-8! succession. saimee =] hmph plis great.... 2-goodbyes knowww crown! rocked!! odyssey, neverrr\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "separate desks, computers, and sides of the room with short dividers...but, we`re thinking we need them to extend to the ceiling\n",
      "\n",
      "Decoded Sentence\n",
      "big pos.. big planted). sj. amos 16-8! succession. saimee =] hmph plis great.... 2-goodbyes knowww crown! rocked!! odyssey, neverrr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this came from the bidirectional model at 100 epochs at 23.18 train loss\n",
    "translate(encoder, decoder, x.to(device), y.to(device), s.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T03:12:28.283583Z",
     "start_time": "2020-06-19T03:12:19.433077Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a51fd9d1940c595ce4548abf2dfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard Similarity: 0.000\n"
     ]
    }
   ],
   "source": [
    "jac_scoring(encoder, decoder, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-19T03:16:30.718Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0af5955ed94635b10f0e3d249d3c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 168.187 Val Loss: 230.240 Time: 26.385\n",
      "Epoch: 20 Train Loss: 115.341 Val Loss: 254.984 Time: 26.591\n",
      "Epoch: 30 Train Loss: 79.362 Val Loss: 284.526 Time: 26.592\n",
      "Epoch: 40 Train Loss: 57.075 Val Loss: 286.976 Time: 26.738\n"
     ]
    }
   ],
   "source": [
    "# continuous training\n",
    "model_path = 'models/multi_bidir/'\n",
    "train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "            model_path + 'bidir_multi_enc_lr_001_i32_h64_ct2.pth',\n",
    "            model_path + 'dec_lr_001_i64_h64_ct2.pth',\n",
    "            0,\n",
    "            50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
