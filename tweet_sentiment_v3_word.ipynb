{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:05.022171Z",
     "start_time": "2020-06-19T21:00:05.017108Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:07.093006Z",
     "start_time": "2020-06-19T21:00:05.813929Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Torch Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:09.337916Z",
     "start_time": "2020-06-19T21:00:08.137541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:09.525069Z",
     "start_time": "2020-06-19T21:00:09.339494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:09.549856Z",
     "start_time": "2020-06-19T21:00:09.526873Z"
    }
   },
   "outputs": [],
   "source": [
    "# index 314 has no text\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:10.290011Z",
     "start_time": "2020-06-19T21:00:10.277672Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val, _, _ = train_test_split(df, \n",
    "                                          df['selected_text'], \n",
    "                                          test_size=.05, \n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:10.373089Z",
     "start_time": "2020-06-19T21:00:10.360279Z"
    }
   },
   "outputs": [],
   "source": [
    "# 26k samples in training to 1k samples in validation\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "* Lowercase - possible since the predicted sentiment text and the selected_text will be lowercase when computing metric.\n",
    "* punctuation - keep the punctuation given that the submission file states that need to be quoted and complete. \n",
    "* Numericalize - Turn each token into its corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:13.389973Z",
     "start_time": "2020-06-19T21:00:13.384407Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    This function will preprocess the input sentence sequence to avoid any further preprocessing\n",
    "    downstream.\n",
    "    \"\"\"\n",
    "    return sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:15.422028Z",
     "start_time": "2020-06-19T21:00:15.084438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47d78eae6bc4868a172acfec695b209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26106.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7a5008f62d4019a7cd37edd4b95b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0e066905884d50a82480834ff1f642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26106.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf334a8f332e48c4a66e7e9814059148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75403751d07484297be3e2fe2b4827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3534.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lowercasing all the text and turning them into a list of tokens for text and selected text in the training set\n",
    "df_train['text'] = df_train['text'].progress_apply(preprocessing)\n",
    "df_val['text'] = df_val['text'].progress_apply(preprocessing)\n",
    "\n",
    "df_train['selected_text'] = df_train['selected_text'].progress_apply(preprocessing)\n",
    "df_val['selected_text'] = df_val['selected_text'].progress_apply(preprocessing)\n",
    "\n",
    "df_test['text'] = df_test['text'].progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:24.554714Z",
     "start_time": "2020-06-19T21:00:24.548389Z"
    }
   },
   "outputs": [],
   "source": [
    "def unique_words(data):\n",
    "    \"\"\"\n",
    "    Find the number of unique words in the training set.\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for text in data:\n",
    "        for word in text:\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:25.467993Z",
     "start_time": "2020-06-19T21:00:25.405444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43859"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_words = unique_words(df_train.text)\n",
    "len(unq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:26.285154Z",
     "start_time": "2020-06-19T21:00:26.277593Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(words):\n",
    "    vocab2idx, idx = {}, 4\n",
    "    vocab2idx['<pad>'] = 0\n",
    "    vocab2idx['<unk>'] = 1\n",
    "    vocab2idx['<sos>'] = 2\n",
    "    vocab2idx['<eos>'] = 3\n",
    "    \n",
    "    for word in words:\n",
    "        vocab2idx[word] = idx\n",
    "        idx += 1\n",
    "    return vocab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:28.468966Z",
     "start_time": "2020-06-19T21:00:28.445868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43863"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2idx = create_vocabulary(unq_words)\n",
    "len(vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:30.926340Z",
     "start_time": "2020-06-19T21:00:30.919377Z"
    }
   },
   "outputs": [],
   "source": [
    "def encoding_with_no_padding(sentence, vocab2idx):\n",
    "    numericalize = [vocab2idx['<sos>']]\n",
    "    for token in sentence:\n",
    "        numericalize.append(vocab2idx.get(token, vocab2idx['<unk>']))\n",
    "    numericalize.append(vocab2idx['<eos>'])\n",
    "    return numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:32.825244Z",
     "start_time": "2020-06-19T21:00:32.815298Z"
    }
   },
   "outputs": [],
   "source": [
    "class tweetDataset(Dataset):\n",
    "    def __init__(self, data, vocab2idx):\n",
    "        self.X = [encoding_with_no_padding(x, vocab2idx) for x in data['text']]\n",
    "        self.y = [encoding_with_no_padding(y, vocab2idx) for y in data['selected_text']]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.X[idx]), torch.LongTensor(self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:34.791347Z",
     "start_time": "2020-06-19T21:00:34.575460Z"
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "train_ds = tweetDataset(df_train, vocab2idx)\n",
    "valid_ds = tweetDataset(df_val, vocab2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:36.025693Z",
     "start_time": "2020-06-19T21:00:36.018720Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    (X, y) = zip(*batch)\n",
    "    x_len = [len(x) for x in X]\n",
    "    x_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_pad = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    return x_pad, x_len, y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:00:38.805087Z",
     "start_time": "2020-06-19T21:00:38.792560Z"
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "batch_size = 3\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "x, lengths, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:05:42.291783Z",
     "start_time": "2020-06-19T21:05:42.280385Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.dropout(self.vocabs(x))\n",
    "        x_pack = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        out, (hidden, cell) = self.lstm(x)\n",
    "        return out, torch.flatten(hidden.transpose(1,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:05:09.973442Z",
     "start_time": "2020-06-19T22:05:09.959954Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocabs = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell, enc_hidden):\n",
    "        x = self.vocabs(x)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        per_hidden = hidden.permute((1,2,0))\n",
    "        scores = torch.bmm(enc_hidden, per_hidden)\n",
    "        alpha = F.softmax(scores, dim=1).permute((0,2,1))  # attention weights\n",
    "        attention = torch.bmm(alpha, enc_hidden).squeeze(1)  # attention vectors\n",
    "        final = torch.cat((attention, per_hidden.squeeze()), dim=1)  # concatenate decoder's hidden state with attention\n",
    "        return self.linear(final), hidden, cell  #NOTE: hidden[-1] not needed because dim = [batch, hidden]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:45:42.505435Z",
     "start_time": "2020-06-19T21:45:42.376612Z"
    }
   },
   "source": [
    "### Encoder-AttentionDecoder Sanity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:05:23.271173Z",
     "start_time": "2020-06-19T22:05:23.133772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 43863]), torch.Size([1, 3, 128]), torch.Size([1, 3, 128]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if it works\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 1)\n",
    "decoder = AttentionDecoder(len(vocab2idx), 64, 128)\n",
    "\n",
    "# quick test of the attention to make sure it runs\n",
    "out, h = encoder(x, lengths)\n",
    "h = h.unsqueeze(0)\n",
    "cell = torch.zeros(h.size())\n",
    "d_out, d_hid, d_cell = decoder(x, h, cell, out)\n",
    "d_out.size(), d_hid.size(), d_cell.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:56:14.308075Z",
     "start_time": "2020-06-19T21:56:14.294674Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "                e_path, d_path, tf_ratio, epochs=10, save_val=False):\n",
    "    best_val = float('inf')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = time.time()\n",
    "        total_loss, total = 0, 0\n",
    "        val_loss, total_v = 0, 0\n",
    "        ### Training\n",
    "        for x, lengths, y in train_dl:\n",
    "            loss = train_batch(encoder, \n",
    "                               decoder, \n",
    "                               enc_optimizer, \n",
    "                               dec_optimizer, \n",
    "                               x.to(device), \n",
    "                               y.to(device),  \n",
    "                               lengths, \n",
    "                               tf=True,\n",
    "                               tf_ratio=tf_ratio)\n",
    "            total_loss += loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        ### Validation\n",
    "        for x, lengths, y in valid_dl:\n",
    "            v_loss = train_batch(encoder, \n",
    "                                 decoder, \n",
    "                                 enc_optimizer, \n",
    "                                 dec_optimizer, \n",
    "                                 x.to(device), \n",
    "                                 y.to(device), \n",
    "                                 lengths, \n",
    "                                 tf=False, \n",
    "                                 train=False)\n",
    "            val_loss += v_loss*x.size(0)\n",
    "            total_v += x.size(0)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1} Train Loss: {total_loss/total:.3f} Val Loss: {val_loss/total_v:.3f} Time: {time.time()-start:.3f}\")\n",
    "        \n",
    "        if save_val:\n",
    "            if best_val > (val_loss/total_v):\n",
    "                save_model(encoder, decoder, e_path, d_path)\n",
    "                best_val = val_loss/total_v\n",
    "        else:\n",
    "            if best_val > (total_loss/total):\n",
    "                save_model(encoder, decoder, e_path, d_path)\n",
    "                best_val = total_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:00:33.508902Z",
     "start_time": "2020-06-19T22:00:33.493936Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(encoder, decoder, enc_optimizer, dec_optimizer, x, y, lengths, tf,\n",
    "                train=True, tf_ratio=0.5):\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "    \n",
    "    # zero grad for both optimizers\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    # **ENCODER**\n",
    "    encoder_outputs, hidden = encoder(x, lengths)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "        y_idx = y[:, idx]\n",
    "        loss += F.cross_entropy(output, y_idx, ignore_index=0)\n",
    "        # if teacher forcing\n",
    "        if tf:\n",
    "            teacher_force = True if np.random.uniform() > tf_ratio else False\n",
    "            if teacher_force:\n",
    "                decoder_input = y_idx.unsqueeze(1)\n",
    "        else:\n",
    "            decoder_input = output.argmax(dim=1).unsqueeze(1)\n",
    "            \n",
    "    # updating the gradient\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T21:54:24.504303Z",
     "start_time": "2020-06-19T21:54:24.496500Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder, e_path, d_path):\n",
    "    torch.save(encoder.state_dict(), e_path)\n",
    "    torch.save(decoder.state_dict(), d_path)\n",
    "    \n",
    "def load_model(encoder, decoder, e_path, d_path):\n",
    "    encoder.load_state_dict(torch.load(e_path))\n",
    "    decoder.load_state_dict(torch.load(d_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:05:41.804778Z",
     "start_time": "2020-06-19T22:05:41.661895Z"
    }
   },
   "outputs": [],
   "source": [
    "# first train of an attention model\n",
    "encoder = Encoder(len(vocab2idx), 32, 64, 1).to(device)\n",
    "decoder = AttentionDecoder(len(vocab2idx), 64, 128).to(device)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:05:42.284536Z",
     "start_time": "2020-06-19T22:05:42.111522Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_ds = tweetDataset(df_train, vocab2idx)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:46:57.381766Z",
     "start_time": "2020-06-19T22:05:43.094401Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d85f86d7a4d549b1b56fff5be131d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 152.374 Val Loss: 206.871 Time: 24.248\n",
      "Epoch: 20 Train Loss: 112.401 Val Loss: 226.791 Time: 24.706\n",
      "Epoch: 30 Train Loss: 84.293 Val Loss: 233.447 Time: 24.691\n",
      "Epoch: 40 Train Loss: 67.612 Val Loss: 238.680 Time: 24.771\n",
      "Epoch: 50 Train Loss: 54.889 Val Loss: 249.034 Time: 24.705\n",
      "Epoch: 60 Train Loss: 44.477 Val Loss: 248.820 Time: 24.142\n",
      "Epoch: 70 Train Loss: 35.854 Val Loss: 254.746 Time: 24.827\n",
      "Epoch: 80 Train Loss: 28.700 Val Loss: 259.474 Time: 24.597\n",
      "Epoch: 90 Train Loss: 23.251 Val Loss: 260.658 Time: 24.629\n",
      "Epoch: 100 Train Loss: 18.794 Val Loss: 263.742 Time: 24.885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models/attention/'\n",
    "train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "            model_path + 'enc_lr_001_i32_h64.pth',\n",
    "            model_path + 'att_dec_lr_001_i64_h128.pth',\n",
    "            0,\n",
    "            100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:47:11.934940Z",
     "start_time": "2020-06-19T22:47:11.909376Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2vocab = {v:k for k, v in list(vocab2idx.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:47:13.219401Z",
     "start_time": "2020-06-19T22:47:13.211672Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(string1, string2):\n",
    "    a = set(string1.lower().split())\n",
    "    b = set(string2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:51:04.955743Z",
     "start_time": "2020-06-19T22:51:04.940907Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoding(encoder, decoder, x, y, lengths):\n",
    "    decoded_words = []\n",
    "    total_jaccard, total = 0, 0\n",
    "    loss = 0\n",
    "    # **ENCODER**\n",
    "    encoder_outputs, hidden = encoder(x, lengths)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "        pred = output.argmax(dim=1)\n",
    "        decoded_words.append(pred)\n",
    "        decoder_input = pred.unsqueeze(1)\n",
    "    \n",
    "    decoded_words = torch.cat([tensor.unsqueeze(0) for tensor in decoded_words]).transpose(1,0)\n",
    "    \n",
    "    for i in range(decoded_words.size(0)):\n",
    "        xi = decoded_words[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        x_sent = ' '.join([idx2vocab[idx] for idx in xi if idx > 3])\n",
    "        y_sent = ' '.join([idx2vocab[idx] for idx in yi if idx > 3])\n",
    "        total_jaccard += jaccard(x_sent, y_sent)\n",
    "        total += y.size(0)\n",
    "    return total_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:48:47.423073Z",
     "start_time": "2020-06-19T22:48:47.414701Z"
    }
   },
   "outputs": [],
   "source": [
    "def jac_scoring(encoder, decoder, data_loader):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    jac_score, total = 0, 0\n",
    "    \n",
    "    for x, lengths, y in tqdm(data_loader):\n",
    "        jac_score += decoding(encoder, decoder, x.to(device), y.to(device), lengths)\n",
    "        total += x.size(0)\n",
    "    print(f\"Jaccard Similarity: {jac_score/total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:51:16.231651Z",
     "start_time": "2020-06-19T22:51:07.527299Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c353e79f074ac297d310d646c9b346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard Similarity: 0.437\n"
     ]
    }
   ],
   "source": [
    "jac_scoring(encoder, decoder, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:53:18.617722Z",
     "start_time": "2020-06-19T22:53:18.612120Z"
    }
   },
   "outputs": [],
   "source": [
    "translation_train_dl = DataLoader(train_ds, batch_size=3, shuffle=True, collate_fn=collate)\n",
    "x, lengths, y = next(iter(translation_train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:54:52.155390Z",
     "start_time": "2020-06-19T22:54:52.141682Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, x, y, lengths):\n",
    "    decoded_words = []\n",
    "    total_jaccard, total = 0, 0\n",
    "    \n",
    "    encoder_outputs, hidden = encoder(x, lengths)  # passing both the sequence and the sentiment\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    cell = torch.zeros(hidden.size()).cuda()\n",
    "    \n",
    "    # **DECODER**\n",
    "    batch_target_length = y.size(1)  # NOTE: the length of the selected text\n",
    "    decoder_input = x[:, 0].unsqueeze(1)\n",
    "    \n",
    "    for idx in range(1, batch_target_length):\n",
    "        output, hidden, cell = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "        pred = output.argmax(dim=1)\n",
    "        decoded_words.append(pred)\n",
    "        decoder_input = pred.unsqueeze(1)\n",
    "    \n",
    "    decoded_words = torch.cat([tensor.unsqueeze(0) for tensor in decoded_words]).transpose(1,0)\n",
    "    \n",
    "    for i in range(decoded_words.size(0)):\n",
    "        xi = decoded_words[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        x_sent = ' '.join([idx2vocab[idx] for idx in xi if idx > 3])\n",
    "        y_sent = ' '.join([idx2vocab[idx] for idx in yi if idx > 3])\n",
    "        print(f\"Ground Truth\")\n",
    "        print()\n",
    "        print(y_sent)\n",
    "        print()\n",
    "        print(f\"Decoded Sentence\")\n",
    "        print(x_sent)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:54:58.969301Z",
     "start_time": "2020-06-19T22:54:58.950437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "greeeeeeeeeeeat\n",
      "\n",
      "Decoded Sentence\n",
      "i have an hour this weekend i\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "i`ll give u an icecream ?\n",
      "\n",
      "Decoded Sentence\n",
      "i`ll u know why\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "stressed\n",
      "\n",
      "Decoded Sentence\n",
      "haha im work and having a pizza\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this came from the bidirectional model at 100 epochs at 23.18 train loss\n",
    "translate(encoder, decoder, x.to(device), y.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:56:02.729560Z",
     "start_time": "2020-06-19T22:56:02.710230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "stupid\n",
      "\n",
      "Decoded Sentence\n",
      "stupid\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "dead.\n",
      "\n",
      "Decoded Sentence\n",
      "omg 44 in\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "feel sad\n",
      "\n",
      "Decoded Sentence\n",
      "yeah it is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, lengths, y = next(iter(translation_train_dl))\n",
    "translate(encoder, decoder, x.to(device), y.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T22:56:24.240921Z",
     "start_time": "2020-06-19T22:56:24.219179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "i love you\n",
      "\n",
      "Decoded Sentence\n",
      "i love you\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "summer is finally here! graduation in a week!\n",
      "\n",
      "Decoded Sentence\n",
      "awesome in a house\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "it`s ace!\n",
      "\n",
      "Decoded Sentence\n",
      "? it`s new songs out and we got new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, lengths, y = next(iter(translation_train_dl))\n",
    "translate(encoder, decoder, x.to(device), y.to(device), lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T23:56:04.710174Z",
     "start_time": "2020-06-19T23:14:55.960929Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce3de919f42438da2cd2dc2174dc56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 15.398 Val Loss: 267.406 Time: 24.809\n",
      "Epoch: 20 Train Loss: 12.965 Val Loss: 274.353 Time: 24.782\n",
      "Epoch: 30 Train Loss: 11.366 Val Loss: 285.141 Time: 24.694\n",
      "Epoch: 40 Train Loss: 9.591 Val Loss: 290.908 Time: 24.691\n",
      "Epoch: 50 Train Loss: 8.610 Val Loss: 289.903 Time: 24.668\n",
      "Epoch: 60 Train Loss: 7.708 Val Loss: 296.292 Time: 24.710\n",
      "Epoch: 70 Train Loss: 8.111 Val Loss: 296.320 Time: 24.075\n",
      "Epoch: 80 Train Loss: 6.161 Val Loss: 310.696 Time: 24.694\n",
      "Epoch: 90 Train Loss: 5.683 Val Loss: 305.082 Time: 24.064\n",
      "Epoch: 100 Train Loss: 5.393 Val Loss: 313.788 Time: 24.527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = 'models/attention/'\n",
    "train_model(encoder, decoder, enc_optimizer, dec_optimizer, train_dl, valid_dl, \n",
    "            model_path + 'ct_enc_lr_001_i32_h64.pth',\n",
    "            model_path + 'ct_att_dec_lr_001_i64_h128.pth',\n",
    "            0,\n",
    "            100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T23:57:13.756248Z",
     "start_time": "2020-06-19T23:57:05.036756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dquan3/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52a105cb4d64b40ad12a5e32490663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jaccard Similarity: 0.613\n"
     ]
    }
   ],
   "source": [
    "jac_scoring(encoder, decoder, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T23:57:36.109538Z",
     "start_time": "2020-06-19T23:57:36.086663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "yea dude... 5`8'! really!?\n",
      "\n",
      "Decoded Sentence\n",
      "yea dude... 5`8'! really!?\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "excited.\n",
      "\n",
      "Decoded Sentence\n",
      "excited. got our politicians night. i`m so i`m so i`m\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "nooo! i`m ill must have paracetamol! nice weather today, though.\n",
      "\n",
      "Decoded Sentence\n",
      "nooo! i`m ill must have paracetamol! nice weather today, though.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, lengths, y = next(iter(translation_train_dl))\n",
    "translate(encoder, decoder, x.to(device), y.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T23:58:47.795865Z",
     "start_time": "2020-06-19T23:58:47.768932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth\n",
      "\n",
      "favourites.\n",
      "\n",
      "Decoded Sentence\n",
      "favourites. and the first was off after there was 2!\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "beer is an excellent excuse. earlier i was sweating god knows how much! not looking forward to working tomorrow\n",
      "\n",
      "Decoded Sentence\n",
      "beer is an excellent excuse. earlier i was sweating god knows how much! not looking forward to working tomorrow\n",
      "\n",
      "Ground Truth\n",
      "\n",
      "super stressful day & can`t sleep...glad i found you your so sweet\n",
      "\n",
      "Decoded Sentence\n",
      "super stressful day & can`t sleep...glad i found you your so sweet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, lengths, y = next(iter(translation_train_dl))\n",
    "translate(encoder, decoder, x.to(device), y.to(device), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
